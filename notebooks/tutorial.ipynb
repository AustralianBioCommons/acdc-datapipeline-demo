{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b32f0721",
   "metadata": {},
   "source": [
    "# Complete Implementation Guide: Patient Data Pipeline with Step Functions, dbt-glue, and AWS Glue\n",
    "\n",
    "Based on our previous conversation, I'll provide you with a comprehensive, step-by-step implementation guide to build this data pipeline. This guide will take you from zero to a working pipeline with detailed instructions for every component.\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "We're building a serverless data pipeline that:\n",
    "1. Accepts Excel files with patient data (with messy `sex` column values)\n",
    "2. Transforms the data using dbt models running on AWS Glue\n",
    "3. Validates data quality with automated tests\n",
    "4. Saves clean data to S3 for downstream consumption\n",
    "5. Orchestrates everything with AWS Step Functions\n",
    "\n",
    "## Phase 1: Repository and Project Structure Setup\n",
    "\n",
    "### Step 1.1: Create the Project Repository\n",
    "\n",
    "First, let's set up your project structure. This will be your main repository.\n",
    "\n",
    "```bash\n",
    "# Create the main project directory\n",
    "mkdir patient-data-pipeline\n",
    "cd patient-data-pipeline\n",
    "\n",
    "# Initialize git repository\n",
    "git init\n",
    "```\n",
    "\n",
    "### Step 1.2: Create the Complete Project Structure\n",
    "\n",
    "Based on dbt best practices[1][2], create this exact folder structure:\n",
    "\n",
    "```bash\n",
    "# Create the main project directories\n",
    "mkdir -p dbt_project/models/staging\n",
    "mkdir -p dbt_project/models/marts\n",
    "mkdir -p dbt_project/tests\n",
    "mkdir -p dbt_project/macros\n",
    "mkdir -p infrastructure/lambda_functions\n",
    "mkdir -p infrastructure/step_functions\n",
    "mkdir -p test_data\n",
    "mkdir -p docs\n",
    "\n",
    "# Create the files we'll need\n",
    "touch dbt_project/dbt_project.yml\n",
    "touch dbt_project/profiles.yml\n",
    "touch infrastructure/requirements.txt\n",
    "touch README.md\n",
    "touch .gitignore\n",
    "```\n",
    "\n",
    "Your final structure should look like this:\n",
    "```\n",
    "patient-data-pipeline/\n",
    "├── dbt_project/\n",
    "│   ├── dbt_project.yml\n",
    "│   ├── profiles.yml\n",
    "│   ├── models│   ├── _sources.yml\n",
    "│   │   │   ├── stg_patient_data.sql\n",
    "│   │   │   └── schema.yml\n",
    "│   │   └── marts/\n",
    "│   │       └── patient_analytics/\n",
    "│   ├── tests/\n",
    "│   ├── macros/\n",
    "│   └── README.md\n",
    "├── infrastructure/\n",
    "│   ├── lambda_functions/\n",
    "│   │   └── dbt_runner/\n",
    "│   │       ├── lambda_function.py\n",
    "│   │       └── requirements.txt\n",
    "│   └── step_functions/\n",
    "│       └── state_machine.json\n",
    "├── test_data/\n",
    "│   └── sample_patient_data.xlsx\n",
    "├── docs/\n",
    "├── README.md\n",
    "└── .gitignore\n",
    "```\n",
    "\n",
    "### Step 1.3: Create Essential Configuration Files\n",
    "\n",
    "Create `.gitignore`:\n",
    "```gitignore\n",
    "# Python\n",
    "__pycache__/\n",
    "*.py[cod]\n",
    "*$py.class\n",
    "*.so\n",
    ".Python\n",
    "env/\n",
    "venv/\n",
    ".venv/\n",
    "pip-log.txt\n",
    "pip-delete-this-directory.txt\n",
    "\n",
    "# dbt\n",
    "target/\n",
    "dbt_packages/\n",
    "logs/\n",
    ".dbt/\n",
    "\n",
    "# AWS\n",
    ".aws/\n",
    "*.zip\n",
    "*.pem\n",
    "\n",
    "# OS\n",
    ".DS_Store\n",
    "Thumbs.db\n",
    "\n",
    "# IDE\n",
    ".vscode/\n",
    ".idea/\n",
    "*.swp\n",
    "*.swo\n",
    "\n",
    "# Terraform (if used later)\n",
    "*.tfstate\n",
    "*.tfstate.backup\n",
    ".terraform/\n",
    "```\n",
    "\n",
    "## Phase 2: Local Development Environment Setup\n",
    "\n",
    "### Step 2.1: Python Environment Setup\n",
    "\n",
    "Based on the latest dbt-glue requirements[3][4], set up your local environment:\n",
    "\n",
    "```bash\n",
    "# Create Python virtual environment\n",
    "python3 -m venv .venv\n",
    "source .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n",
    "\n",
    "# Install required packages\n",
    "pip install dbt-core==1.8.0\n",
    "pip install dbt-glue>=1.8.0\n",
    "pip install boto3\n",
    "pip install pandas\n",
    "pip install openpyxl  # For Excel file handling\n",
    "```\n",
    "\n",
    "Create `infrastructure/requirements.txt`:\n",
    "```txt\n",
    "dbt-core==1.8.0\n",
    "dbt-glue>=1.8.0\n",
    "boto3>=1.26.0\n",
    "pandas>=1.5.0\n",
    "openpyxl>=3.1.0\n",
    "```\n",
    "\n",
    "### Step 2.2: AWS CLI Configuration\n",
    "\n",
    "Ensure your AWS CLI is configured with appropriate credentials:\n",
    "\n",
    "```bash\n",
    "# Configure AWS CLI (if not already done)\n",
    "aws configure\n",
    "\n",
    "# Test your connection\n",
    "aws sts get-caller-identity\n",
    "```\n",
    "\n",
    "## Phase 3: dbt Project Setup\n",
    "\n",
    "### Step 3.1: Create dbt Project Configuration\n",
    "\n",
    "Create `dbt_project/dbt_project.yml`:\n",
    "```yaml\n",
    "name: 'patient_data_pipeline'\n",
    "version: '1.0.0'\n",
    "config-version: 2\n",
    "\n",
    "# This setting configures which \"profile\" dbt uses for this project.\n",
    "profile: 'patient_data_pipeline'\n",
    "\n",
    "# These configurations specify where dbt should look for different types of files.\n",
    "model-paths: [\"models\"]\n",
    "analysis-paths: [\"analyses\"]\n",
    "test-paths: [\"tests\"]\n",
    "seed-paths: [\"seeds\"]\n",
    "macro-paths: [\"macros\"]\n",
    "snapshot-paths: [\"snapshots\"]\n",
    "\n",
    "clean-targets:\n",
    "  - \"target\"\n",
    "  - \"dbt_packages\"\n",
    "\n",
    "models:\n",
    "  patient_data_pipeline:\n",
    "    # Configuration for staging models\n",
    "    staging:\n",
    "      +materialized: view\n",
    "      +tags: [\"staging\"]\n",
    "    \n",
    "    # Configuration for mart models  \n",
    "    marts:\n",
    "      +materialized: table\n",
    "      +tags: [\"marts\"]\n",
    "      patient_analytics:\n",
    "        +schema: patient_analytics\n",
    "```\n",
    "\n",
    "### Step 3.2: Create dbt Profile Configuration\n",
    "\n",
    "Create `dbt_project/profiles.yml` based on the dbt-glue setup guide[3][5]:\n",
    "```yaml\n",
    "patient_data_pipeline:\n",
    "  target: dev\n",
    "  outputs:\n",
    "    dev:\n",
    "      type: glue\n",
    "      role_arn: \"arn:aws:iam::YOUR_ACCOUNT_ID:role/GlueInteractiveSessionRole\"  # You'll create this in Phase 4\n",
    "      region: us-east-1  # Change to your preferred region\n",
    "      workers: 2\n",
    "      worker_type: G.1X\n",
    "      idle_timeout: 10\n",
    "      glue_version: \"4.0\"\n",
    "      schema: \"patient_data_dev\"\n",
    "      location: \"s3://patient-data-lake-YOUR_ACCOUNT_ID/processed/\"  # You'll create this bucket\n",
    "      glue_session_reuse: true\n",
    "      session_provisioning_timeout_in_seconds: 120\n",
    "```\n",
    "\n",
    "### Step 3.3: Create dbt Models\n",
    "\n",
    "Create `dbt_project/models/staging/_sources.yml`:\n",
    "```yaml\n",
    "version: 2\n",
    "\n",
    "sources:\n",
    "  - name: raw_data\n",
    "    description: \"Raw patient data from Excel uploads\"\n",
    "    tables:\n",
    "      - name: patient_excel_data\n",
    "        description: \"Patient data with messy sex column values (0,1,M,F,male,female)\"\n",
    "        columns:\n",
    "          - name: patient_id\n",
    "            description: \"Unique patient identifier\"\n",
    "            tests:\n",
    "              - not_null\n",
    "              - unique\n",
    "          - name: sex\n",
    "            description: \"Sex column with mixed formats that need standardization\"\n",
    "```\n",
    "\n",
    "Create `dbt_project/models/staging/stg_patient_data.sql`:\n",
    "```sql\n",
    "{{ config(\n",
    "    materialized='table',\n",
    "    tags=['staging', 'patient_data']\n",
    ") }}\n",
    "\n",
    "WITH source_data AS (\n",
    "    SELECT \n",
    "        patient_id,\n",
    "        sex AS raw_sex,\n",
    "        CURRENT_TIMESTAMP() AS processed_at,\n",
    "        '{{ run_started_at }}' AS dbt_run_timestamp\n",
    "    FROM {{ source('raw_data', 'patient_excel_data') }}\n",
    "),\n",
    "\n",
    "cleaned_data AS (\n",
    "    SELECT \n",
    "        patient_id,\n",
    "        CASE \n",
    "            WHEN UPPER(TRIM(raw_sex)) IN ('M', 'MALE', '1') THEN 'M'\n",
    "            WHEN UPPER(TRIM(raw_sex)) IN ('F', 'FEMALE', '0') THEN 'F'\n",
    "            ELSE NULL\n",
    "        END AS sex,\n",
    "        raw_sex AS original_sex_value,\n",
    "        processed_at,\n",
    "        dbt_run_timestamp\n",
    "    FROM source_data\n",
    ")\n",
    "\n",
    "SELECT * \n",
    "FROM cleaned_data\n",
    "WHERE sex IS NOT NULL  -- Only keep records with valid sex values\n",
    "```\n",
    "\n",
    "Create `dbt_project/models/staging/schema.yml`:\n",
    "```yaml\n",
    "version: 2\n",
    "\n",
    "models:\n",
    "  - name: stg_patient_data\n",
    "    description: \"Cleaned and standardized patient data\"\n",
    "    columns:\n",
    "      - name: patient_id\n",
    "        description: \"Unique patient identifier\"\n",
    "        tests:\n",
    "          - not_null\n",
    "          - unique\n",
    "        \n",
    "      - name: sex\n",
    "        description: \"Patient sex standardized to M or F\"\n",
    "        tests:\n",
    "          - not_null\n",
    "          - accepted_values:\n",
    "              values: ['M', 'F']\n",
    "              quote: true\n",
    "        \n",
    "      - name: original_sex_value\n",
    "        description: \"Original raw value before transformation\"\n",
    "        \n",
    "      - name: processed_at\n",
    "        description: \"Timestamp when the record was processed\"\n",
    "        tests:\n",
    "          - not_null\n",
    "```\n",
    "\n",
    "### Step 3.4: Create Test Data\n",
    "\n",
    "Create `test_data/sample_patient_data.xlsx` with this content (create in Excel or using Python):\n",
    "\n",
    "```python\n",
    "# Run this to create test data\n",
    "import pandas as pd\n",
    "\n",
    "test_data = {\n",
    "    'patient_id': ['P001', 'P002', 'P003', 'P004', 'P005', 'P006', 'P007'],\n",
    "    'sex': ['M', 'F', '1', '0', 'male', 'female', 'MALE']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(test_data)\n",
    "df.to_excel('test_data/sample_patient_data.xlsx', index=False)\n",
    "print(\"Test data created successfully!\")\n",
    "```\n",
    "\n",
    "## Phase 4: AWS Infrastructure Setup\n",
    "\n",
    "### Step 4.1: Create IAM Roles\n",
    "\n",
    "You'll need several IAM roles. Create these using the AWS Console or CLI.\n",
    "\n",
    "**IAM Role 1: Glue Interactive Session Role**\n",
    "\n",
    "Based on AWS Glue IAM requirements[6][7], create this role:\n",
    "\n",
    "1. Go to AWS IAM Console → Roles → Create Role\n",
    "2. Choose \"Glue\" as the service\n",
    "3. Attach these policies:\n",
    "   - `AWSGlueServiceRole`\n",
    "   - `AmazonS3FullAccess` (or a more restrictive policy)\n",
    "4. Name it: `GlueInteractiveSessionRole`\n",
    "\n",
    "**IAM Role 2: Lambda Execution Role**\n",
    "\n",
    "1. Create role for Lambda service\n",
    "2. Attach these policies:\n",
    "   - `AWSLambdaBasicExecutionRole`\n",
    "   - `AWSGlueConsoleFullAccess`\n",
    "   - `AmazonS3ReadOnlyAccess`\n",
    "3. Name it: `PatientDataLambdaRole`\n",
    "\n",
    "**IAM Role 3: Step Functions Execution Role**\n",
    "\n",
    "1. Create role for Step Functions service\n",
    "2. Attach these policies:\n",
    "   - `AWSStepFunctionsFullAccess`\n",
    "   - `AWSLambdaInvokeFunction`\n",
    "3. Name it: `PatientDataStepFunctionsRole`\n",
    "\n",
    "### Step 4.2: Create S3 Buckets\n",
    "\n",
    "Create the required S3 buckets:\n",
    "\n",
    "```bash\n",
    "# Replace YOUR_ACCOUNT_ID with your actual AWS account ID\n",
    "export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Create raw data bucket\n",
    "aws s3 mb s3://patient-data-raw-${AWS_ACCOUNT_ID}\n",
    "\n",
    "# Create processed data bucket  \n",
    "aws s3 mb s3://patient-data-lake-${AWS_ACCOUNT_ID}\n",
    "\n",
    "# Create clean data bucket\n",
    "aws s3 mb s3://patient-data-clean-${AWS_ACCOUNT_ID}\n",
    "\n",
    "# Enable EventBridge notifications on the raw bucket\n",
    "aws s3api put-bucket-notification-configuration \\\n",
    "  --bucket patient-data-raw-${AWS_ACCOUNT_ID} \\\n",
    "  --notification-configuration '{\n",
    "    \"EventBridgeConfiguration\": {}\n",
    "  }'\n",
    "```\n",
    "\n",
    "### Step 4.3: Create AWS Glue Database\n",
    "\n",
    "```bash\n",
    "# Create Glue database for our data\n",
    "aws glue create-database \\\n",
    "  --database-input '{\n",
    "    \"Name\": \"patient_data_dev\",\n",
    "    \"Description\": \"Database for patient data pipeline development\"\n",
    "  }'\n",
    "```\n",
    "\n",
    "## Phase 5: Lambda Function Creation\n",
    "\n",
    "### Step 5.1: Create the dbt Runner Lambda Function\n",
    "\n",
    "Create `infrastructure/lambda_functions/dbt_runner/lambda_function.py`:\n",
    "\n",
    "```python\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import boto3\n",
    "import logging\n",
    "from typing import Dict, Any\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def lambda_handler(event: Dict[str, Any], context) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Lambda function to run dbt transformations using AWS Glue Interactive Sessions\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract S3 details from the event\n",
    "        s3_bucket = event.get('s3Bucket', '')\n",
    "        s3_key = event.get('s3Key', '')\n",
    "        \n",
    "        logger.info(f\"Processing file: s3://{s3_bucket}/{s3_key}\")\n",
    "        \n",
    "        # Set up environment for dbt\n",
    "        os.environ['DBT_PROFILES_DIR'] = '/tmp'\n",
    "        os.environ['AWS_DEFAULT_REGION'] = os.environ.get('AWS_REGION', 'us-east-1')\n",
    "        \n",
    "        # Ensure the Excel data is accessible in Glue Data Catalog\n",
    "        setup_glue_table(s3_bucket, s3_key)\n",
    "        \n",
    "        # Run dbt debug to verify connection\n",
    "        debug_result = run_dbt_command(['dbt', 'debug', '--profiles-dir', '/tmp'])\n",
    "        if debug_result['return_code'] != 0:\n",
    "            raise Exception(f\"dbt debug failed: {debug_result['stderr']}\")\n",
    "        \n",
    "        # Run dbt transformations\n",
    "        run_result = run_dbt_command([\n",
    "            'dbt', 'run', \n",
    "            '--models', 'stg_patient_data',\n",
    "            '--profiles-dir', '/tmp'\n",
    "        ])\n",
    "        \n",
    "        # Run dbt tests\n",
    "        test_result = run_dbt_command([\n",
    "            'dbt', 'test', \n",
    "            '--models', 'stg_patient_data',\n",
    "            '--profiles-dir', '/tmp'\n",
    "        ])\n",
    "        \n",
    "        # Check results\n",
    "        transformation_success = run_result['return_code'] == 0\n",
    "        tests_passed = test_result['return_code'] == 0\n",
    "        \n",
    "        response = {\n",
    "            'statusCode': 200,\n",
    "            'transformationSuccess': transformation_success,\n",
    "            'testsPass': tests_passed,\n",
    "            'runOutput': run_result.get('stdout', ''),\n",
    "            'testOutput': test_result.get('stdout', ''),\n",
    "            'processedFile': f\"s3://{s3_bucket}/{s3_key}\"\n",
    "        }\n",
    "        \n",
    "        if not (transformation_success and tests_passed):\n",
    "            response['errors'] = {\n",
    "                'run_errors': run_result.get('stderr', ''),\n",
    "                'test_errors': test_result.get('stderr', '')\n",
    "            }\n",
    "            \n",
    "        logger.info(f\"Pipeline completed. Success: {transformation_success and tests_passed}\")\n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in lambda_handler: {str(e)}\")\n",
    "        return {\n",
    "            'statusCode': 500,\n",
    "            'error': str(e),\n",
    "            'transformationSuccess': False,\n",
    "            'testsPass': False\n",
    "        }\n",
    "\n",
    "def run_dbt_command(command: list) -> Dict[str, Any]:\n",
    "    \"\"\"Run a dbt command and capture output\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            command,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=900,  # 15 minute timeout\n",
    "            cwd='/tmp'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'return_code': result.returncode,\n",
    "            'stdout': result.stdout,\n",
    "            'stderr': result.stderr\n",
    "        }\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return {\n",
    "            'return_code': 1,\n",
    "            'stdout': '',\n",
    "            'stderr': 'Command timed out after 15 minutes'\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'return_code': 1,\n",
    "            'stdout': '',\n",
    "            'stderr': str(e)\n",
    "        }\n",
    "\n",
    "def setup_glue_table(bucket: str, key: str) -> None:\n",
    "    \"\"\"\n",
    "    Set up Glue table to make Excel data accessible to dbt\n",
    "    This is a simplified version - in production, you'd want more robust table creation\n",
    "    \"\"\"\n",
    "    glue_client = boto3.client('glue')\n",
    "    \n",
    "    # Extract table name from S3 key\n",
    "    table_name = 'patient_excel_data'\n",
    "    database_name = 'patient_data_dev'\n",
    "    \n",
    "    try:\n",
    "        # Create or update Glue table pointing to the Excel file\n",
    "        # Note: This assumes the Excel file has been converted to Parquet\n",
    "        # In a real implementation, you'd have a separate process to convert Excel to Parquet\n",
    "        \n",
    "        table_input = {\n",
    "            'Name': table_name,\n",
    "            'StorageDescriptor': {\n",
    "                'Columns': [\n",
    "                    {'Name': 'patient_id', 'Type': 'string'},\n",
    "                    {'Name': 'sex', 'Type': 'string'}\n",
    "                ],\n",
    "                'Location': f's3://{bucket}/processed/',\n",
    "                'InputFormat': 'org.apache.hadoop.mapred.TextInputFormat',\n",
    "                'OutputFormat': 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat',\n",
    "                'SerdeInfo': {\n",
    "                    'SerializationLibrary': 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Try to update existing table, create if it doesn't exist\n",
    "        try:\n",
    "            glue_client.update_table(\n",
    "                DatabaseName=database_name,\n",
    "                TableInput=table_input\n",
    "            )\n",
    "        except glue_client.exceptions.EntityNotFoundException:\n",
    "            glue_client.create_table(\n",
    "                DatabaseName=database_name,\n",
    "                TableInput=table_input\n",
    "            )\n",
    "            \n",
    "        logger.info(f\"Glue table {table_name} set up successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error setting up Glue table: {str(e)}\")\n",
    "        raise\n",
    "```\n",
    "\n",
    "Create `infrastructure/lambda_functions/dbt_runner/requirements.txt`:\n",
    "```txt\n",
    "dbt-core==1.8.0\n",
    "dbt-glue>=1.8.0\n",
    "boto3>=1.26.0\n",
    "```\n",
    "\n",
    "### Step 5.2: Deploy the Lambda Function\n",
    "\n",
    "Package and deploy the Lambda function:\n",
    "\n",
    "```bash\n",
    "cd infrastructure/lambda_functions/dbt_runner\n",
    "\n",
    "# Create deployment package\n",
    "zip -r ../../../dbt-runner-lambda.zip . -x \"*.pyc\" \"__pycache__/*\"\n",
    "\n",
    "# Add your dbt project to the Lambda package\n",
    "cd ../../../dbt_project\n",
    "zip -r ../dbt-runner-lambda.zip . -x \"target/*\" \"dbt_packages/*\" \"logs/*\"\n",
    "\n",
    "cd ..\n",
    "\n",
    "# Deploy Lambda function\n",
    "aws lambda create-function \\\n",
    "  --function-name patient-data-dbt-runner \\\n",
    "  --runtime python3.11 \\\n",
    "  --role arn:aws:iam::YOUR_ACCOUNT_ID:role/PatientDataLambdaRole \\\n",
    "  --handler lambda_function.lambda_handler \\\n",
    "  --zip-file fileb://dbt-runner-lambda.zip \\\n",
    "  --timeout 900 \\\n",
    "  --memory-size 1024 \\\n",
    "  --environment Variables='{\n",
    "    \"AWS_DEFAULT_REGION\":\"us-east-1\"\n",
    "  }'\n",
    "```\n",
    "\n",
    "## Phase 6: Step Functions Workflow Creation\n",
    "\n",
    "### Step 6.1: Create State Machine Definition\n",
    "\n",
    "Create `infrastructure/step_functions/state_machine.json` based on Step Functions best practices[8][9]:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"Comment\": \"Patient Data Processing Pipeline with dbt and AWS Glue\",\n",
    "  \"StartAt\": \"ProcessWithDBT\",\n",
    "  \"States\": {\n",
    "    \"ProcessWithDBT\": {\n",
    "      \"Type\": \"Task\",\n",
    "      \"Resource\": \"arn:aws:states:::lambda:invoke\",\n",
    "      \"Parameters\": {\n",
    "        \"FunctionName\": \"arn:aws:lambda:us-east-1:YOUR_ACCOUNT_ID:function:patient-data-dbt-runner\",\n",
    "        \"Payload\": {\n",
    "          \"s3Bucket.$\": \"$.detail.bucket.name\",\n",
    "          \"s3Key.$\": \"$.detail.object.key\"\n",
    "        }\n",
    "      },\n",
    "      \"ResultPath\": \"$.dbtResult\",\n",
    "      \"Next\": \"CheckProcessingResults\",\n",
    "      \"Retry\": [\n",
    "        {\n",
    "          \"ErrorEquals\": [\"States.TaskFailed\"],\n",
    "          \"IntervalSeconds\": 30,\n",
    "          \"MaxAttempts\": 2,\n",
    "          \"BackoffRate\": 2.0\n",
    "        }\n",
    "      ],\n",
    "      \"Catch\": [\n",
    "        {\n",
    "          \"ErrorEquals\": [\"States.ALL\"],\n",
    "          \"Next\": \"ProcessingFailed\",\n",
    "          \"ResultPath\": \"$.error\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    \n",
    "    \"CheckProcessingResults\": {\n",
    "      \"Type\": \"Choice\",\n",
    "      \"Choices\": [\n",
    "        {\n",
    "          \"And\": [\n",
    "            {\n",
    "              \"Variable\": \"$.dbtResult.Payload.transformationSuccess\",\n",
    "              \"BooleanEquals\": true\n",
    "            },\n",
    "            {\n",
    "              \"Variable\": \"$.dbtResult.Payload.testsPass\", \n",
    "              \"BooleanEquals\": true\n",
    "            }\n",
    "          ],\n",
    "          \"Next\": \"ExportCleanData\"\n",
    "        }\n",
    "      ],\n",
    "      \"Default\": \"ProcessingFailed\"\n",
    "    },\n",
    "    \n",
    "    \"ExportCleanData\": {\n",
    "      \"Type\": \"Task\",\n",
    "      \"Resource\": \"arn:aws:states:::aws-sdk:s3:copyObject\",\n",
    "      \"Parameters\": {\n",
    "        \"Bucket\": \"patient-data-clean-YOUR_ACCOUNT_ID\",\n",
    "        \"CopySource\": {\n",
    "          \"Bucket\": \"patient-data-lake-YOUR_ACCOUNT_ID\",\n",
    "          \"Key.$\": \"States.Format('processed/stg_patient_data/{}', $.detail.object.key)\"\n",
    "        },\n",
    "        \"Key.$\": \"States.Format('processed/{}/clean_patient_data.parquet', $.detail.object.key)\"\n",
    "      },\n",
    "      \"Next\": \"Success\",\n",
    "      \"ResultPath\": \"$.exportResult\",\n",
    "      \"Catch\": [\n",
    "        {\n",
    "          \"ErrorEquals\": [\"States.ALL\"],\n",
    "          \"Next\": \"ExportFailed\",\n",
    "          \"ResultPath\": \"$.exportError\"\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    \n",
    "    \"Success\": {\n",
    "      \"Type\": \"Succeed\",\n",
    "      \"Comment\": \"Pipeline completed successfully\"\n",
    "    },\n",
    "    \n",
    "    \"ProcessingFailed\": {\n",
    "      \"Type\": \"Fail\",\n",
    "      \"Error\": \"ProcessingError\",\n",
    "      \"Cause\": \"dbt transformation or testing failed\"\n",
    "    },\n",
    "    \n",
    "    \"ExportFailed\": {\n",
    "      \"Type\": \"Fail\", \n",
    "      \"Error\": \"ExportError\",\n",
    "      \"Cause\": \"Failed to export clean data to final bucket\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### Step 6.2: Create the State Machine\n",
    "\n",
    "```bash\n",
    "# Create the state machine\n",
    "aws stepfunctions create-state-machine \\\n",
    "  --name patient-data-pipeline \\\n",
    "  --definition file://infrastructure/step_functions/state_machine.json \\\n",
    "  --role-arn arn:aws:iam::YOUR_ACCOUNT_ID:role/PatientDataStepFunctionsRole\n",
    "```\n",
    "\n",
    "## Phase 7: EventBridge Rule Setup\n",
    "\n",
    "### Step 7.1: Create EventBridge Rule\n",
    "\n",
    "Based on S3 to EventBridge integration patterns[10][11], create the rule:\n",
    "\n",
    "```bash\n",
    "# Create EventBridge rule to trigger Step Functions on S3 uploads\n",
    "aws events put-rule \\\n",
    "  --name patient-data-s3-trigger \\\n",
    "  --event-pattern '{\n",
    "    \"source\": [\"aws.s3\"],\n",
    "    \"detail-type\": [\"Object Created\"],\n",
    "    \"detail\": {\n",
    "      \"bucket\": {\n",
    "        \"name\": [\"patient-data-raw-'${AWS_ACCOUNT_ID}'\"]\n",
    "      },\n",
    "      \"object\": {\n",
    "        \"key\": [{\n",
    "          \"suffix\": \".xlsx\"\n",
    "        }]\n",
    "      }\n",
    "    }\n",
    "  }' \\\n",
    "  --state ENABLED\n",
    "\n",
    "# Add Step Functions as target\n",
    "aws events put-targets \\\n",
    "  --rule patient-data-s3-trigger \\\n",
    "  --targets '[{\n",
    "    \"Id\": \"1\",\n",
    "    \"Arn\": \"arn:aws:states:us-east-1:'${AWS_ACCOUNT_ID}':stateMachine:patient-data-pipeline\",\n",
    "    \"RoleArn\": \"arn:aws:iam::'${AWS_ACCOUNT_ID}':role/PatientDataStepFunctionsRole\"\n",
    "  }]'\n",
    "```\n",
    "\n",
    "## Phase 8: Testing the Complete Pipeline\n",
    "\n",
    "### Step 8.1: Prepare Test Environment\n",
    "\n",
    "First, test your dbt setup locally:\n",
    "\n",
    "```bash\n",
    "cd dbt_project\n",
    "\n",
    "# Test dbt connection\n",
    "dbt debug --profiles-dir .\n",
    "\n",
    "# If debug passes, try running models (this will fail initially until AWS resources are set up)\n",
    "dbt run --profiles-dir . --models stg_patient_data\n",
    "```\n",
    "\n",
    "### Step 8.2: Test the Complete Pipeline\n",
    "\n",
    "1. **Upload test data to trigger the pipeline:**\n",
    "\n",
    "```bash\n",
    "# Upload the test Excel file\n",
    "aws s3 cp test_data/sample_patient_data.xlsx s3://patient-data-raw-${AWS_ACCOUNT_ID}/test_uploads/sample_patient_data.xlsx\n",
    "```\n",
    "\n",
    "2. **Monitor the execution:**\n",
    "\n",
    "```bash\n",
    "# Check Step Functions executions\n",
    "aws stepfunctions list-executions \\\n",
    "  --state-machine-arn arn:aws:states:us-east-1:${AWS_ACCOUNT_ID}:stateMachine:patient-data-pipeline \\\n",
    "  --max-items 5\n",
    "```\n",
    "\n",
    "3. **Check the results:**\n",
    "\n",
    "```bash\n",
    "# List processed files\n",
    "aws s3 ls s3://patient-data-clean-${AWS_ACCOUNT_ID}/processed/ --recursive\n",
    "\n",
    "# Check Glue Data Catalog\n",
    "aws glue get-tables --database-name patient_data_dev\n",
    "```\n",
    "\n",
    "## Phase 9: Troubleshooting and Monitoring\n",
    "\n",
    "### Step 9.1: Common Issues and Solutions\n",
    "\n",
    "1. **dbt connection issues:**\n",
    "   - Verify IAM roles have correct permissions\n",
    "   - Check that Glue database exists\n",
    "   - Ensure S3 buckets are accessible\n",
    "\n",
    "2. **Lambda timeout issues:**\n",
    "   - Increase Lambda timeout to 15 minutes\n",
    "   - Check CloudWatch logs for detailed errors\n",
    "\n",
    "3. **Step Functions failures:**\n",
    "   - Review execution history in Step Functions console\n",
    "   - Check EventBridge rule patterns match your S3 bucket\n",
    "\n",
    "### Step 9.2: Monitoring Setup\n",
    "\n",
    "Create CloudWatch dashboards to monitor:\n",
    "- Lambda function execution times and errors\n",
    "- Step Functions execution success/failure rates  \n",
    "- S3 bucket object counts\n",
    "- Glue job execution metrics\n",
    "\n",
    "## Next Steps and Enhancements\n",
    "\n",
    "Once your basic pipeline is working, consider these enhancements:\n",
    "\n",
    "1. **Add data validation with Great Expectations integration**\n",
    "2. **Implement data lineage tracking**\n",
    "3. **Add automated data quality monitoring**\n",
    "4. **Set up CI/CD for dbt model deployment**\n",
    "5. **Add human-in-the-loop approval for data releases**\n",
    "6. **Implement data versioning and rollback capabilities**\n",
    "\n",
    "This implementation provides a solid foundation for a production-ready data pipeline that can handle real-world patient data processing requirements while maintaining data quality and security.\n",
    "\n",
    "[1] https://www.thedataschool.co.uk/curtis-paterson/organising-a-dbt-project-best-practices/\n",
    "[2] https://hevodata.com/data-transformation/dbt-best-practices/\n",
    "[3] https://docs.getdbt.com/docs/core/connect-data-platform/glue-setup\n",
    "[4] https://pypi.org/project/dbt-glue/\n",
    "[5] https://aws.amazon.com/blogs/big-data/build-your-data-pipeline-in-your-aws-modern-data-platform-using-aws-lake-formation-aws-glue-and-dbt-core/\n",
    "[6] https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\n",
    "[7] https://docs.aws.amazon.com/glue/latest/dg/glue-is-security.html\n",
    "[8] https://docs.aws.amazon.com/step-functions/latest/dg/concepts-amazon-states-language.html\n",
    "[9] https://docs.aws.amazon.com/en_us/step-functions/latest/dg/concepts-amazon-states-language.html\n",
    "[10] https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-cloudwatch-events-s3.html\n",
    "[11] https://repost.aws/questions/QUw_gvpQ5tSAez6mg7FdwOjg/s3-trigger-for-step-function-using-event-bridge\n",
    "[12] https://aws.amazon.com/blogs/big-data/build-and-manage-your-modern-data-stack-using-dbt-and-aws-glue-through-dbt-glue-the-new-trusted-dbt-adapter/\n",
    "[13] https://dzone.com/articles/orchestrating-dbt-workflows-the-duel-of-apache-air\n",
    "[14] https://www.mechanicalrock.io/blog/automating-data-workflows-with-aws-step-functions-an-example-using-fivetran-and-dbt\n",
    "[15] https://www.thedataschool.co.uk/curtis-paterson/organising-a-dbt-project-best-practices\n",
    "[16] https://www.secoda.co/learn/how-to-set-up-aws-glue-with-dbt-developer-hub\n",
    "[17] https://dev.to/virajlakshitha/orchestrating-the-cloud-building-robust-workflows-with-aws-step-functions-3574\n",
    "[18] https://jaehyeon.me/blog/2022-10-09-dbt-on-aws-part-2-glue/\n",
    "[19] https://www.packtpub.com/en-us/product/data-engineering-with-aws-second-edition-9781804614426/chapter/orchestrating-the-data-pipeline-12/section/hands-on-orchestrating-a-data-pipeline-using-aws-step-functions-ch12lvl1sec85\n",
    "[20] https://docs.getdbt.com/best-practices/how-we-mesh/mesh-3-structures\n",
    "[21] https://www.jeeviacademy.com/how-to-use-aws-step-functions-to-orchestrate-workflows/\n",
    "[22] https://docs.getdbt.com/best-practices/how-we-structure/5-the-rest-of-the-project\n",
    "[23] https://github.com/AirLiquide/al-dbt-glue\n",
    "[24] https://docs.aws.amazon.com/step-functions/latest/dg/sample-lambda-orchestration.html\n",
    "[25] https://docs.getdbt.com/best-practices/how-we-structure/1-guide-overview\n",
    "[26] https://www.youtube.com/watch?v=B7wfWCpfIdQ\n",
    "[27] https://aws.amazon.com/step-functions/\n",
    "[28] https://stackoverflow.com/questions/71255224/how-to-run-dbt-in-aws-lambda\n",
    "[29] https://docs.aws.amazon.com/AWSCloudFormation/latest/TemplateReference/aws-resource-stepfunctions-statemachine.html\n",
    "[30] https://docs.getdbt.com/reference/resource-configs/glue-configs\n",
    "[31] https://aws.amazon.com/blogs/big-data/create-a-modern-data-platform-using-the-data-build-tool-dbt-in-the-aws-cloud/\n",
    "[32] https://www.getorchestra.io/guides/dlt-concepts-deployment-on-aws-lambda\n",
    "[33] https://www.linkedin.com/pulse/dbt-integration-aws-glue-runbook-chaitanya-varma\n",
    "[34] https://discourse.getdbt.com/t/dbt-cloud-webhook-with-aws-lambda/10776\n",
    "[35] https://docs.aws.amazon.com/step-functions/latest/dg/concepts-statemachines.html\n",
    "[36] https://docs.aws.amazon.com/step-functions/latest/dg/input-output-example.html\n",
    "[37] https://docs.snowplow.io/docs/modeling-your-data/modeling-your-data-with-dbt/dbt-configuration/\n",
    "[38] https://www.datacamp.com/tutorial/aws-lambda\n",
    "[39] https://asecure.cloud/a/p_stepfunctions_step_functions_state_machine_with_definitionsubstitutions/\n",
    "[40] https://docs.aws.amazon.com/mwaa/latest/userguide/samples-dbt.html\n",
    "[41] https://docs.aws.amazon.com/step-functions/latest/dg/concepts-input-output-filtering.html\n",
    "[42] https://github.com/TheDataFoundryAU/dbt_sample_project\n",
    "[43] https://dev.to/pizofreude/study-notes-421-422-dbt-project-setup-2d4d\n",
    "[44] https://repost.aws/questions/QUIPd2s9UoTkGIMiVCepHHjg/how-to-trigger-a-step-function-from-a-s3-object-notification\n",
    "[45] https://docs.getdbt.com/docs/build/projects\n",
    "[46] https://www.youtube.com/watch?v=vpDOkl4X0Dc\n",
    "[47] https://joonsolutions.com/build-dbt-project-from-scratch/\n",
    "[48] https://docs.aws.amazon.com/step-functions/latest/dg/using-user-notifications-sfn.html\n",
    "[49] https://aws.amazon.com/about-aws/whats-new/2023/09/aws-glue-interactive-sessions-kernel-support-iam-conditionals/\n",
    "[50] https://docs.aws.amazon.com/lambda/latest/dg/with-s3-example.html\n",
    "[51] https://tuanchris.com/blog/2021-10-11-anatomy-of-a-dbt-project/\n",
    "[52] https://cevo.com.au/post/dbt-on-aws-part-2/\n",
    "[53] https://stackoverflow.com/questions/58221024/trigger-aws-step-function-state-machine-with-s3-event-from-a-different-aws-accou\n",
    "[54] https://articles.xebia.com/birth-of-dbt-excel-adapter\n",
    "[55] https://xebia.com/blog/implementing-aws-cdk-cicd-with-cdk-pipelines/\n",
    "[56] https://github.com/duckdb/dbt-duckdb\n",
    "[57] https://dev.classmethod.jp/articles/s3-event-eventbridge-rule-step-functions/\n",
    "[58] https://github.com/jbcodeforce/aws-cdk-project-templates\n",
    "[59] https://stackoverflow.com/questions/63490730/using-external-parquet-tables-in-a-dbt-pipeline\n",
    "[60] https://docs.aws.amazon.com/step-functions/latest/dg/eventbridge-integration.html\n",
    "[61] https://github.com/danmgs/AWS.Pipeline.CloudFormation\n",
    "[62] https://docs.getdbt.com/reference/resource-configs/duckdb-configs\n",
    "[63] https://dev.to/zirkelc/eventbridge-rules-to-invoke-lambda-and-stepfunction-584m\n",
    "[64] https://dev.to/annpastushko/create-serverless-data-pipeline-using-aws-cdk-python-5cg2\n",
    "[65] https://pypi.org/project/dbt-excel/\n",
    "[66] https://www.youtube.com/watch?v=f_uNbo2cunk\n",
    "[67] https://blog.serverlessadvocate.com/serverless-aws-cdk-pipeline-best-practices-patterns-part-1-ab80962f109d\n",
    "[68] https://stackoverflow.com/questions/64152343/convert-excel-to-parquet-file\n",
    "[69] https://docs.aws.amazon.com/step-functions/latest/dg/service-quotas.html\n",
    "[70] https://github.com/bhanotblocker/CDKTemplates\n",
    "[71] https://github.com/AlexanderVR/dbt-parquet\n",
    "[72] https://serverlessland.com/patterns/s3-eventbridge-sfn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fb995b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
