{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "617efb31",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "To switch to dbt profile with the `arn:aws:iam::026090528544:role/GlueInteractiveSessionClientRole-dbt-CLI-demo` role that I created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ed738e",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "nano ~/.aws/config\n",
    "\n",
    "# In nano add this profile\n",
    "[profile dbt-glue]\n",
    "role_arn = arn:aws:iam::026090528544:role/GlueInteractiveSessionClientRole-dbt-CLI-demo\n",
    "source_profile = Joshua_Harris_ETL\n",
    "region = ap-southeast-2\n",
    "\n",
    "# switch profile for the terminal session\n",
    "export AWS_PROFILE=dbt-glue\n",
    "\n",
    "# check your cureent profile\n",
    "aws sts get-caller-identity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df70d1db",
   "metadata": {},
   "source": [
    "lets make a csv file with example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "693415d3",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# pip install pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87fd58b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from numpy import nan\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def permute_dataframe(df, random_state=None):\n",
    "    \"\"\"\n",
    "    Returns a new DataFrame with the rows randomly permuted.\n",
    "    Each call produces a different order unless random_state is set.\n",
    "    \"\"\"\n",
    "    return df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "\n",
    "\n",
    "dirty_df = DataFrame({\n",
    "    'patient_id': [\"A001\", \"A002\", \"A003\", \"A004\", \"A005\", \"\"], \n",
    "    'sex': [\"M\", \"F\", \"0\", \"1\", \"Male\", \"fem\"],\n",
    "    'medication': [None, '', None, nan, None, 'statin'],\n",
    "    'age': [nan, 32, 214, 21, 0, \"\"]\n",
    "    })\n",
    "\n",
    "\n",
    "# Example usage of permute_dataframe:\n",
    "\n",
    "import random\n",
    "permuted_df = permute_dataframe(dirty_df, random_state=random.randint(0, 1_000_000))\n",
    "\n",
    "\n",
    "with open('../data/patient_info.csv', 'w') as f:\n",
    "    permuted_df.to_csv(f, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e761a0aa",
   "metadata": {},
   "source": [
    "lets upload this file to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0986e2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ../data/patient_info.csv to s3://acdc-dbt-test-raw/data/patient_info/20250725173021/patient_info.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "DATETIME=$(date +%Y%m%d%H%M%S)\n",
    "DATA_NAME=\"patient_info\"\n",
    "aws s3 cp \"../data/patient_info.csv\" \"s3://acdc-dbt-test-raw/data/${DATA_NAME}/${DATETIME}/${DATA_NAME}.csv\" --profile \"default\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f709f46a",
   "metadata": {},
   "source": [
    "lets make some other data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9121b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "from numpy import nan\n",
    "\n",
    "dirty_df = DataFrame({\n",
    "    'sample_id': [\"SAM0001\", \"SAM0002\", \"SAM0003\", \"SAM0004\", \"SAM0005\", \"\"],\n",
    "    'patient_id': [\"A001\", \"A002\", \"A003\", \"A004\", \"A005\", \"\"], \n",
    "    'storage_medium': [\"nitrogen\", \"air\", \"nitrogen\", \"air\", \"nitrogen\", \"air\"],\n",
    "    'storage_date': [\"2022-01-01\", \"2022-01-02\", \"2022-01-03\", \"2022-01-04\", \"2022-01-05\", \"2022-01-06\"],\n",
    "    'volume_ul': [0, 486, 4, 4000, 5000, 6000]\n",
    "    })\n",
    "\n",
    "permuted_df = permute_dataframe(dirty_df, random_state=random.randint(0, 1_000_000))\n",
    "\n",
    "with open('../data/sample.csv', 'w') as f:\n",
    "    permuted_df.to_csv(f, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc9a2992",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ../data/sample.csv to s3://acdc-dbt-test-raw/data/sample/20250725173021/sample.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "DATETIME=$(date +%Y%m%d%H%M%S)\n",
    "DATA_NAME=\"sample\"\n",
    "aws s3 cp \"../data/sample.csv\" \"s3://acdc-dbt-test-raw/data/${DATA_NAME}/${DATETIME}/${DATA_NAME}.csv\" --profile \"default\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d67d809",
   "metadata": {},
   "source": [
    "# Trying to run dbt\n",
    "\n",
    "### IAM roles:\n",
    "\n",
    "The principle role: `arn:aws:iam::026090528544:role/GlueInteractiveSessionClientRole-dbt-CLI-demo` as the following policies attached:\n",
    "-  `arn:aws:iam::026090528544:policy/acdc-dbt-glue-test`\n",
    "-  `arn:aws:iam::026090528544:policy/GlueInteractiveSessionClientPolicy-dbt`\n",
    "        - This role Grants permission to create and manage AWS Glue interactive sessions and pass the designated execution role required to run dbt workloads in AWS Glue.\n",
    "\n",
    "Once glue has been created, then `AwsGlueSessionUserRestrictedServiceRole-dbtGlueExecutor` role which is defined in `profiles.yml` will then be used by the glue engine to run spark statements. This role contains the following policies:\n",
    "- `arn:aws:iam::aws:policy/service-role/AwsGlueSessionUserRestrictedServiceRole`\n",
    "- `arn:aws:iam::026090528544:policy/acdc-dbt-glue-test`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e46613",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf60e158",
   "metadata": {},
   "source": [
    "### 🔐 IAM Roles for `dbt` + AWS Glue Interactive Sessions\n",
    "\n",
    "Running `dbt` on AWS Glue using interactive sessions requires **two IAM roles**, each with a distinct purpose:\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. 🧑‍💻 Client Principal Role\n",
    "\n",
    "**Role ARN:**  \n",
    "`arn:aws:iam::026090528544:role/GlueInteractiveSessionClientRole-dbt-CLI-demo`\n",
    "\n",
    "This role is **assumed by you** (or your CI/CD runner) when you execute `dbt` commands from the CLI. It represents the identity that **creates and manages** interactive Glue sessions.\n",
    "\n",
    "**Attached Policies:**\n",
    "- `arn:aws:iam::026090528544:policy/acdc-dbt-glue-test`\n",
    "- `arn:aws:iam::026090528544:policy/GlueInteractiveSessionClientPolicy-dbt`\n",
    "\n",
    "**What this role allows:**\n",
    "- Creating Glue sessions:  \n",
    "  `glue:CreateSession`, `glue:GetSession`, `glue:CancelStatement`, `glue:RunStatement`\n",
    "- Passing execution roles to the Glue service:  \n",
    "  `iam:PassRole`\n",
    "\n",
    "> 💡 *Think of this as the **\"session initiator\"** — the identity that opens the workspace and tells Glue what role to use inside.*\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. ⚙️ Executor (Runtime) Role\n",
    "\n",
    "**Role ARN (defined in `profiles.yml`):**  \n",
    "`arn:aws:iam::026090528544:role/AwsGlueSessionUserRestrictedServiceRole-dbtGlueExecutor`\n",
    "\n",
    "This role is **assumed by Glue** after a session is created. It is used by the Spark engine to **execute your dbt workload inside Glue**.\n",
    "\n",
    "**Attached Policies:**\n",
    "- `arn:aws:iam::aws:policy/service-role/AwsGlueSessionUserRestrictedServiceRole`\n",
    "- `arn:aws:iam::026090528544:policy/acdc-dbt-glue-test`\n",
    "\n",
    "**What this role allows:**\n",
    "- Accessing AWS Glue Data Catalog:  \n",
    "  `glue:GetDatabase`, `glue:GetTable`, `glue:GetPartition`, etc.\n",
    "- Accessing and writing to S3:  \n",
    "  `s3:GetObject`, `s3:PutObject`, `s3:ListBucket`\n",
    "\n",
    "> 💡 *Think of this as the **\"Glue worker\"** — the identity that Spark uses to run your SQL models and materialize results.*\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Summary Table\n",
    "\n",
    "| Role Name                                               | Used By       | Purpose                      | Key Permissions                             |\n",
    "|----------------------------------------------------------|----------------|-------------------------------|----------------------------------------------|\n",
    "| `GlueInteractiveSessionClientRole-dbt-CLI-demo`          | dbt CLI / You | Create and manage Glue sessions | `glue:*Session*`, `glue:RunStatement`, `iam:PassRole` |\n",
    "| `AwsGlueSessionUserRestrictedServiceRole-dbtGlueExecutor` | AWS Glue       | Run Spark code inside session | `glue:Get*`, `s3:*`, Spark runtime           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "16598391",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Checking dbt status to see if it connected and ready\n",
    "# !dbt debug # Run in terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9924b5",
   "metadata": {},
   "source": [
    "Should look like:\n",
    "\n",
    "```\n",
    "dbt debug                                                                                                                                                                                                                                                                                                            (main✱) \n",
    "04:33:16  Running with dbt=1.9.8\n",
    "04:33:16  dbt version: 1.9.8\n",
    "04:33:16  python version: 3.9.5\n",
    "04:33:16  python path: /Users/harrijh/.pyenv/versions/3.9.5/bin/python3.9\n",
    "04:33:16  os info: macOS-15.5-arm64-arm-64bit\n",
    "04:33:16  Using profiles dir at /Users/harrijh/projects/acdc-datapipeline-demo/dbt_demo_acdc\n",
    "04:33:16  Using profiles.yml file at /Users/harrijh/projects/acdc-datapipeline-demo/dbt_demo_acdc/profiles.yml\n",
    "04:33:16  Using dbt_project.yml file at /Users/harrijh/projects/acdc-datapipeline-demo/dbt_demo_acdc/dbt_project.yml\n",
    "04:33:16  adapter type: glue\n",
    "04:33:16  adapter version: 1.9.4\n",
    "04:33:16  Configuration:\n",
    "04:33:16    profiles.yml file [OK found and valid]\n",
    "04:33:16    dbt_project.yml file [OK found and valid]\n",
    "04:33:16  Required dependencies:\n",
    "04:33:16   - git [OK found]\n",
    "\n",
    "04:33:16  Connection:\n",
    "04:33:16    role_arn: arn:aws:iam::026090528544:role/AwsGlueSessionUserRestrictedServiceRole-dbtGlueExecutor\n",
    "04:33:16    region: ap-southeast-2\n",
    "04:33:16    workers: 2\n",
    "04:33:16    worker_type: G.1X\n",
    "04:33:16    session_provisioning_timeout_in_seconds: 120\n",
    "04:33:16    schema: glue-db-acdc-dbt-test-clean\n",
    "04:33:16    location: s3://acdc-dbt-test-clean/dbt/\n",
    "04:33:16    extra_jars: None\n",
    "04:33:16    idle_timeout: 10\n",
    "04:33:16    query_timeout_in_minutes: 300\n",
    "04:33:16    glue_version: 5.0\n",
    "04:33:16    security_configuration: None\n",
    "04:33:16    connections: None\n",
    "04:33:16    conf: None\n",
    "04:33:16    extra_py_files: None\n",
    "04:33:16    delta_athena_prefix: None\n",
    "04:33:16    tags: None\n",
    "04:33:16    seed_format: parquet\n",
    "04:33:16    seed_mode: overwrite\n",
    "04:33:16    default_arguments: None\n",
    "04:33:16    iceberg_glue_commit_lock_table: myGlueLockTable\n",
    "04:33:16    use_interactive_session_role_for_api_calls: False\n",
    "04:33:16    lf_tags: None\n",
    "04:33:16    glue_session_id: None\n",
    "04:33:16    glue_session_reuse: True\n",
    "04:33:16    datalake_formats: None\n",
    "04:33:16    enable_session_per_model: False\n",
    "04:33:16    use_arrow: False\n",
    "04:33:16    enable_spark_seed_casting: False\n",
    "04:33:16  Registered adapter: glue=1.9.4\n",
    "04:33:50    Connection test: [OK connection ok]\n",
    "\n",
    "04:33:50  All checks passed!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0711e5ca",
   "metadata": {},
   "source": [
    "running my new model `dbt_demo_acdc/models/staging/stg_patient_info.sql`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99e2125c",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# dbt run --select stg_patient_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f24429",
   "metadata": {},
   "source": [
    "# Here is what DBT is doing with versioning\n",
    "\n",
    "Based on the advanced, versioned project structure we've designed, here is a detailed breakdown of what will happen when you run the command `dbt run --select stg_patient_info`.\n",
    "\n",
    "This command now triggers a more sophisticated process because you've implemented dbt model versioning and a dynamic, immutable S3 release path.\n",
    "\n",
    "### The Step-by-Step Execution Flow\n",
    "\n",
    "When you execute `dbt run --select stg_patient_info`, dbt will perform the following steps:\n",
    "\n",
    "1.  **Model Resolution**: The command `stg_patient_info` refers to the *conceptual model*. dbt will look at your `models/staging/patient_info/_patient_info__models.yml` file to determine which version of this model to run. It sees the line `latest_version: 1`, so it knows to execute the logic defined for `v1`.\n",
    "\n",
    "2.  **Code Execution**: dbt finds the `v1` model, which points to the file `stg_patient_info_v1.sql`. It will execute the SQL transformation logic contained within that specific file.\n",
    "\n",
    "3.  **Dynamic Path Construction**: dbt then looks at your `profiles.yml` to determine where to save the output. It finds the `location` parameter:\n",
    "    `location: \"s3://acdc-dbt-test-clean/releases/{{ var('release_version', 'default') }}/\"`\n",
    "    It then checks your `dbt_project.yml` for the value of the `release_version` variable, which is set to `\"v1.0\"`.\n",
    "\n",
    "4.  **Final Output Path**: dbt constructs the final, immutable S3 path for this run, which will be:\n",
    "    `s3://acdc-dbt-test-clean/releases/v1.0/stg_patient_info/`\n",
    "\n",
    "### The Two Possible Outcomes\n",
    "\n",
    "What happens next depends entirely on whether this is the first time you are creating the `v1.0` release.\n",
    "\n",
    "#### Scenario A: This is the first time you are creating the \"v1.0\" release\n",
    "\n",
    "If the directory `s3://acdc-dbt-test-clean/releases/v1.0/` does not yet exist, the command will **succeed**.\n",
    "\n",
    "*   The AWS Glue job will run.\n",
    "*   The `stg_patient_info_v1.sql` logic will transform the data.\n",
    "*   The clean data will be written as new Parquet files to the new `.../releases/v1.0/stg_patient_info/` folder.\n",
    "*   A table definition for `stg_patient_info` will be created in your `glue_db_acdc_dbt_test_clean` database, pointing to this new, versioned S3 location.\n",
    "\n",
    "#### Scenario B: You have already created the \"v1.0\" release before\n",
    "\n",
    "If you have already run this command successfully once, the directory `.../releases/v1.0/stg_patient_info/` will already exist and contain data. In this case, your `dbt run` command will **fail** with the same error you encountered previously[1][2][3]:\n",
    "\n",
    "`AnalysisException: CREATE-TABLE-AS-SELECT cannot create table with location to a non-empty directory...`\n",
    "\n",
    "This is the expected safety feature preventing you from accidentally corrupting a data release.\n",
    "\n",
    "### How to Proceed in Scenario B\n",
    "\n",
    "If you encounter the \"non-empty directory\" error for your `v1.0` release and you intentionally want to rebuild it from scratch, you must use the `--full-refresh` flag[4].\n",
    "\n",
    "```bash\n",
    "dbt run --select stg_patient_info --full-refresh\n",
    "```\n",
    "\n",
    "This command tells dbt to first delete the existing data files in `s3://.../releases/v1.0/stg_patient_info/` before running the `CREATE TABLE` command, ensuring the run will succeed.\n",
    "\n",
    "By implementing this versioned structure, you have established a robust pattern for managing your data releases. Each new version of your data will live in its own immutable S3 folder, creating a clear, auditable history that is perfect for reproducible research.\n",
    "\n",
    "[1] https://community.databricks.com/t5/data-engineering/error-the-associated-location-is-not-empty-but-it-s-not-a-delta/td-p/7428\n",
    "[2] https://github.com/databricks/dbt-databricks/issues/771\n",
    "[3] https://github.com/trinodb/trino/issues/20749\n",
    "[4] https://hevodata.com/data-transformation/dbt-full-refresh/\n",
    "[5] https://stackoverflow.com/questions/77472158/iceberg-filesystem-error-cannot-create-a-table-on-a-non-empty-location\n",
    "[6] https://discourse.getdbt.com/t/create-empty-table-through-dbt-seed/11667\n",
    "[7] https://docs.getdbt.com/reference/resource-properties/versions\n",
    "[8] https://www.linkedin.com/pulse/mastering-breaking-changes-safely-removing-columns-dbt-joshua-odeyemi-5xskf\n",
    "[9] https://www.getgalaxy.io/learn/glossary/dbt-snapshot\n",
    "[10] https://www.datafold.com/blog/catching-unintended-changes-to-immutable-data\n",
    "[11] https://community.databricks.com/t5/get-started-discussions/bug-cannot-create-table-the-associated-location-is-not-empty-and/td-p/37335\n",
    "[12] https://popsql.com/learn-dbt/dbt-full-refresh\n",
    "[13] https://altisconsulting.com/au/insights/leveraging-dbt-model-versions-to-enhance-trust-and-reliability/\n",
    "[14] https://handbook.gitlab.com/handbook/enterprise-data/how-we-work/dbt-change-workflow/\n",
    "[15] https://discourse.getdbt.com/t/model-version-of-a-snapshot/17677\n",
    "[16] https://www.getdbt.com/blog/introducing-release-tracks-for-dbt-version-upgrades\n",
    "[17] https://www.datacamp.com/tutorial/databricks-create-table\n",
    "[18] https://docs.getdbt.com/docs/build/incremental-models\n",
    "[19] https://docs.getdbt.com/docs/mesh/govern/model-versions\n",
    "[20] https://discourse.getdbt.com/t/dbt-support-for-schema-versioned-database-objects/6131"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229ae79b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
